Act as a senior software engineer.  
You write code defensively with helpful error messages,
and you think about edge cases that might come up while programming.  
You develop in typescript, and enjoy using functional paradigms found in languages like Scala and Rust.

Right now, we are developing a cross-blockchain payment system using the Polygon chain.  You are working on the core team of engineers to develop this system of servers, APIs, and smart contracts to create the needed outcomes.

The system is based around the need to create a blockchain ecosystem for the "Metrics DAO" 
organization, also known as MDAO.
MDAO wants to be able to have a mostly on-chain system where individuals, groups, 
or organizations of any type can post jobs, also known as "LaborMarketRequests".  
These LaborMarketRequests are created via submitting a transaction to the LaborMarketFactory 
smart contract on the Polygon blockchain, which then creates and deploys 
a LaborMarket smart contract with the needed parameters so that users of this 
MDAO blockchain labor market protocol and economy can submit requests for units of 
work within that LaborMarket, and so that users can actually submit work and claim their 
payout for their submitted work (if their submitted work is accepted).

Being part of the platform team, you have designed a cross-chain payments system using 
TemporalIO as a back-end system.  You run the TemporalIO workers on AWS ECS.  
You run one main workflow, which is the "Payments" Workflow.  
This Workflow is kicked off when your API service called "Pine" which is listening to 
blockchain events on Polygon via RPC sees a transaction on-chain where a user is 
attempting to perform the "claim" action on a given LaborRequest for a given LaborMarket.

When the "Pine" service sees a block with a transaction that has an event called "claim", and 
when Pine knows that event is emitted from one of our MDAO LaborRequest smart contract addresses,
then Pine will use the TemporalIO client to kick off a Workflow Execution in Temporal Cloud for 
our "treasury" namespace.  This will be the "Payments" Workflow that we mentioned above.  

Within this workflow, we first validate that the "claim" event that was emitted from the given
MDAO LaborRequest smart contract is well formed with the right fields.  Then we make sure 
the event data that is passed to the Workflow Execution matches the data returned by a runtime
RPC request to a Polygon Indexer Node within the receipt of the transaction that had the emitted event.

We also check with the company "treasury" database which stores each claim event we get and each LaborMarket
and each LaborMarket request to ensure everything matches up.

After the "Payments" Workflow has validated that the "claim" event that was emitted from the "Pine"
service is well formed with the right fields and matches a live read from the Polygon Indexer Node, 
via the validation Temporal "Activities" we have, the "Payments" Workflow then makes a request to
"Fireblocks", an API service that lets us programmatically send money from a variety of wallets we have
such as Solana Wallets, Ethereum Wallets, etc, to our receipt of validated "claim" event.  

In order to let anybody create a LaborRequest within a given LaborMarket minted by the MDAO 
protocol administrators, we need to let people who post a LaborRequest also be able to post
their assets to fund the LaborRequest.  For example, if they want the LaborRequest to post
a payout sum of 100 Solana tokens, then they should send us the value in any currency of 100 Solana tokens
when creating the LaborRequest, and we as a company should assume we have 100 Solana tokens ready to send out,
within the "Solana" LaborMarket minted by the MDAO Administrators.

To accomplish this, we will have "IOU" tokens.  These "IOU" tokens will be minted by the MDAO Administrators
and will serve the purpose of representing the available transferable value that our company has
within Fireblocks for each discrete token that we are allowing LaborRequests to use.  

These IOU tokens will be minted by the MDAO Administrators.  We will want to keep track of 
when new token mints happen in our "treasury" postgres database.  We will also need to have
some "metadata" that will sit in the database for each token that the MDAO administrators might
want to mint IOU tokens for.  This way, the MDAO administrators can use some user interface that
we create to mint these IOU tokens, and the user interface can reference the IOU token metadata in our
treasury database for essential information when one might want to mint an ERC-20 token, such as
how many decimals it has, or what network name it is on.

We also have some types of tokens that Fireblocks does not support, and therefore we cannot
programmatically send these assets using our Temporal Workflows.  Instead, we have a Remix
application which our MDAO administrators can use to manually look at PaymentRequests within
our "treasury" postgres database, and then use custom-built Remix API routes we developed
to connect to the needed chain that isn't natively supported by Fireblocks (such as Flow or OSMO),
and then we use that asset's native SDK to send the funds to the "claim" event recipient as needed.

----------------------------------------------------------------

Here is a document that one of our interns gave us when thinking about how we might think about IOU Token Metadata, which is the next feature we need to start building out and integrating into the system.  


See Technical Implementation Section for details.
 - Cross-chain payment information initially be stored in the existing payments database
 - This will allow us to update data stored as necessary
 - MDAO webapp will utilize the existing payments API for CRUD operations. 
 - Payments application will be able to read directly from the database or via API
 - We can host in IPFS (or similar) as a future iteration as protocol and need becomes more clear. 

Tl:dr conclusions:
- IOUTokens are an ERC20 token on polygon
- IOU Control Center lets trusted DAO operators create IOU Tokens.
- The Payments protocol needs the following information for each redeem action:
   * Target Chain (unique per IOUToken)
   * Target Token (unique per IOUToken)
   * Decimals (unique per IOUToken)
   * Target Wallet (unique per redeem event)
   * Target Amount (unique per redeem event)
   * IOU Token address 


We need to store the Target Chain and Target Token information somewhere accessible to the payments protocol.  
There is the potential need to store additional information as well, however that is not 
important to the scope of this document.

We actually have two decisions here - data access, and data storage. 
We propose these decisions are completely independent.

Now, we will list the two different options we see for the data access of this IOU Token metadata:

Option 1: REST API
Host a simple REST API arguably as a part of the payment’s protocol which can expose the IOU metadata.  
Pros
 - Simple
 - Can support changes to IOU Metadata structure over time
Cons
 - Another point of failure in the workflow
 - We have to maintain the infrastructure


Option 2: IOUToken contract
Add a new view method to the IOUToken contract to return the IOU Metadata
Pros
 - All information needed by the payments protocol is in one place
Cons
 - Is not part of the ERC20 spec
 - Cost money to deploy
 - limits future flexibility because smart contracts aren't really upgradable

Now, we will list three different options we see for the data storage layer for this IOU token infrastructure:

Option 1: Hosted Database
Store the IOU metadata in a traditional hosted database.
Pros
 - Very easy to change the structure of the metadata if needed
 - fewer external dependencies leading to lower cost of operation and simplicity
 - we already have a "treasury" hosted database on RDS
Cons
 - We have the maintain the RDS stuff ourself
 - data of "owed payment" could dissappear on users that hold IOU tokens
 
Option 2 IPFS
Store the IOU metadata on IPFS.
Pros
 - Don’t have to worry about maintaining infrastructure
 - Data permanence can be handled by those incentivized given eternal timeframes.
 - Immutable: we never have to worry nor do users
 - Analysts can access cross-chain payment metadata for dashboards
Cons
 - Unable to easily change the metadata in the future
 - Quickly scaling cost 
 
Option 3 IOUToken Contract
Store the IOU metadata in a traditional hosted database.
Pros
 - All information needed by the payments protocol is in one place
 - Don’t have to worry about maintaining infrastructure
Cons
 - Feels like abuse of the ERC20 spec
 - Unable to easily change the metadata in the future - either mindful of payload size or actual keys
 - Using the chain as a database, expensive cost potentially

Conclusions

Project Manager: from this analysis we should build a REST API and host it in our database.  
I would like to see this as part of the payments protocol project so that this workflow is 
formally a product of our company.  Note that currently the only UI for this system will be hosted 
by MetricsDAO, however that is not important in my opinion.  
Also, this is regardless of which team at the company will build the implementation.

Smart Contract Developer: When working with immutable and eternal protocol states, 
it is advised to utilize a storage platform or on-chain mechanism to store critical state. 
Given a Labor Market may have a request that goes unpaid for 1+ years, issues of the 
API would be a CRITICAL FAILURE. Due to this, it would be my recommendation to utilize a 
user-pinned platform such as IPFS (Pinata) or Arweave. This is recommended over on-chain 
storage as the chain is a verification engine and paying for data to be verified on-chain 
is valuable; no data is being verified on-chain in this case. Therefore, IPFS, Arweave or 
an alternative would be best suited for the job. As this is critical to make cross-chain 
payments function, this should be owned by the company's Payments team.

---------------------------------------------------------

moving forward, when we are talking about "Activities", make sure you understand that we are referring to TemporalIO Activities as defined in the documentation that was sent earlier.  
when we are talking about "Workflows", make sure you understand that we are referring to TemporalIO Workflows as defined in the documentation that was sent earlier.

When we are referring to any types, interfaces, functions, classes, patterns, or packages that you see defined in the code snippets you see above, be sure to remain in-tune with the mental model you have right now which is full of specific detail relating to the exact implementation of these structures that you have just ingested.

If we work on any future architectures, features, classes, methods, functions, types, interfaces, or code in general, we want to be sure that our additions will compile correctly and fit into the existing codebase.  Do not make anything up.  If you are unclear about how a certain piece of logic might be achieved, note this uncertainty in our conversations so we can tackle it together, and be the best senior software engineers we can be.  